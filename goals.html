<!DOCTYPE html>

<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width">
		<meta name="description" content="Jeremiah Wernham">
		<meta name="keywords" content="Jeremiah Wernham, Software Engineering (Co-op) student, University of Guelph">
		<meta name="author" content="Jeremiah Wernham">
		<title>Goals | Co-op Workterm Report</title>
		<link rel="stylesheet" href="./css/style.css">
	</head>
	<body>
		<div id="page-container">
			<div id="content-wrap">
				<header>
					<div class="container">
						<div id="branding">
							<h1><span class="highlight">Co-op </span>Workterm Report</h1>
						</div>
						<nav>
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="about.html">About</a></li>
								<li class ="current"><a href="goals.html">Goals</a></li>
								<li><a href="conclusion.html">Conclusion</a></li>
							</ul>
						</nav>
					</div>
				</header>

				<section id="divider">
						<div class="container">
							<div id="name">
								<h4>Jeremiah Wernham | Software Engineering (CO-OP)</h4>
							</div>
							<div id="contact">
								<h4>jwernham@uoguelph.ca</h4>
							</div>
						</div>
				</section>

		        <section id="main">
		            <div class="container">
		                <h1 class="page-title">Goals</h1>
		                	<ul id="goals">
		                		<li>
		                			<h2>CRITICAL & CREATIVE THINKING - Depth & Breadth of Understanding </h2>
													<p>Create Dashboards using Splunk Cloud to visualize machine data from Camis application, website, and SQL databases managed in Azure to provide clear metrics for DevOps and IT Systems teams. </p>
													<h3>Action Plan</h3>
		                			<p>Splunk Cloud is a web-based application for searching, monitoring, and analyzing machine-generated big data. This data is indexed into searchable repositories from which it can generate graphs, reports, alerts, dashboards, and visualizations. Creating dashboards is possible through the Splunk Search Language and some UI elements. To learn the Splunk Search Language I will follow the Splunk Fundamentals course, technical documentation, and online forums. I will work closely with the DevOps and Systems teams to gain a clear understanding if the infrastructure and operations needed to report on, test, and maintain the Camis applications and websites in use for each client across North America. I will also employ version control software to develop my dashboards incrementally, incorporating feedback along the way, and document my changes to allow for further development after my co-op term.</p>
													<h3>Measure of Success</h3>
													<p>My measure of success in this endeavor is to first gain a firm understanding of the Splunk Search Language by completing the Splunk Fundamentals course within the first week of my term. Once complete, I can also create an article in Confluence to document what I've learned for my teammates to reference. I then need to determine what relations would be best to graph for each type of machine data being indexed. Each dashboard I create would have 5 to 8 statistics tables or visualizations to provide a clear understanding of metrics for the given collection, but not overload the viewer with information. I aim to have the Azure SQL Audit Logs Dashboard up and running 3 weeks into my work term, Azure Web Logs 4 weeks in, and Azure App Logs 5 weeks in. This dashboard may be considered completed within the time range but can also be updated as more metrics are needed throughout the term.</p>
													<h3>Reflection</h3>
													<p>Over the past 4 months, I have created multiple Splunk dashboards to better aid the team in visualizing site traffic before my set deadlines. I've completed Splunk Fundamentals courses I and II and gained extensive knowledge of the Splunk Search Language through my own research using Splunk's technical documentation. Working with the Systems team, I have created 3 dashboards related to scanning potentially malicious internet traffic for all of our clients' sites. The visualizations I developed includes reporting on monthly vulnerability scans for performed by an external security company, and reporting on botting activity, such as when, by what device, and by what endpoints where impacted. Changes made to dashboards have been uploaded to our version control software, and documentation has been written as I develop these dashboards iteratively. I have gained a clear understanding if the infrastructure and operations needed to report on, test, and maintain Camis applications. As a result, I have achieved this goal.</p>
		                		</li>
		                		
		                		<li>
		                			<h2>LITERACY - Technological Literacy</h2>
													<p>Advance my technological understanding of common troubleshooting requests that DevOps receives by familiarizing myself with the terms used in describing the issue and actioning them in day-to-day work.</p>
													<h3>Action Plan</h3>
		                			<p>Part of the roles of DevOps includes the responsibility up maintaining the technologies used to automate production and resolving issues as they're reported. Currently, common troubleshooting requests that DevOps receives are a database being out of date, a Camis application or supported website being slow, or throwing errors, and emails not being auto generated. These are reported to us mainly internally by employees in the company, or by automated tasks to alert us when something is wrong, but also externally through emails which we monitor. To expand my technical vocabulary, I will work directly with technologies pertaining to the issue and ask questions should I not understand what something means. By resolving these issues as they're assigned to me, or by picking them up on my own, I will more quickly understand what should be done and begin to deliberate with my team about the best solution.</p>
													<h3>Measure of Success</h3>
													<p>Within 2 months of my co-op term I aim to have a clear understanding of the technical components that may be causing or affected by each of the common problems discussed above, and identify the problems simply based on the terms used to describe it. In order to achieve this, I will have needed to communicate technical information to teammates about the issue and work with them to find the best solution. Following this, I aim to move on to more advanced issues, even resolving problems in the Camis production environment should they arrive. Finally, I aim to develop methods of preventing issues from occurring and see that no more requests to DevOps are created for an issue because of this.</p>
													<h3>Reflection</h3>
													<p>My measure of success for this goal was to gain a clear understanding of any technical components that may be causing common troubleshooting requests that Dev-Ops receives. My initial learning towards completing this goal was greatly benefitted by documentation that was created within this work term specifically for some of the errors DevOps must deal with. Team meetings were held which included demonstrations of what to do should these problems arise which helped me as well. I unfortunately had not been able to work directly with employees encountering these issues, as the range of errors reported to DevOps is wide and better handled with urgency by experienced members of the team. However, I have been monitoring communication of issues as they are worked on to gain a better understanding of how the problem was addressed. As I become more proficient in my main duties of implementing changes to the build pipeline, I aim to explore these troubleshooting tasks further and develop methods of preventing them.
													</p>
		                		</li>

		                		<li>
		                			<h2>CRITICAL & CREATIVE THINKING - Problem Solving</h2>
													<p>Use Windows PowerShell to solve problems for reporting on and scaling company infrastructure according to real-time site traffic, and in automating Camis product build pipeline.</p>
													<h3>Action Plan</h3>
		                			<p>The main goal of DevOps as a set of practices is to shorten the systems development life cycle and provide continuous delivery with high software quality. At Camis, the DevOps team uses a set of tools related to coding, building, testing, deploying, and monitoring the applications that Camis creates, referred to as the build pipeline. In order to shorten the systems development lifecycle, and ensure the software that is continually in development maintains a high level of quality, code that is ready to be made part of the working application passes through the pipeline and is only released once it's determined viable. All of this is done in an automated process and much of the backbone of this process is Windows PowerShell. PowerShell is a powerful scripting language that allows simplification and automation of tedious, repetitive tasks using scripts and commands. For example, scaling the operational power of the virtual databases, and app services hosted by Microsoft Azure, is scheduled according to the usual patterns of traffic to Camis applications in order to reduce unnecessary costs using PowerShell. However, it is sometimes difficult to predict the amount incoming traffic to a site when a client first opens campsite booking for the season. Therefore, my action plan is to create PowerShell scripts to continuously monitor traffic on these sites when large amounts of bookings occur, and then scale the operational power appropriately, maintaining a high level of software quality.</p>
													<h3>Measure of Success</h3>
													<p>My measure of success for this endeavor is to first adapt current PowerShell scripts used to monitor Azure app services and databases to instead report real time data continuously to the command line within 3 months of my work term. In order to do that I will need to gain a firm understanding of campsite booking and payment operations and why they are affected by a high level of traffic. I will then create scripts to track their response times at different levels of operational capacity. Within 5 months of my work term I aim for my code to function properly with a web application and database in Azure and to provide a demonstration with a server in the testing environment.  By the end of my work term I hope to have my code working to scale operational capacity with real traffic in a production environment. Ultimately, I would have gathered a solid understanding of how PowerShell can be used to automate production following the main practices of DevOps.</p>
													<h3>Reflection</h3>
													<p>During my work term, I have been able to greatly advance my understanding of both cloud computing, and how to use Azure's CLI tool in PowerShell to manage our clients' app services programmatically. This goal specifically challenged my ability to learn quickly in the role of a DevOps engineer, and bring forward my existing knowledge of PowerShell to implement a cost-saving, powerfully predictive auto-scaler tool. To advance my knowledge of Azure, I volunteered to be enrolled in the online Azure Fundamentals course. Upon completion, I was confident in assigning myself a ticket that required me to use the Azure CLI tool with PowerShell, in scheduling when a database is to be moved into and out of an elastic pool (for cost saving measures). As my understanding for these technologies grew, I also began to understand why the current method of scheduling when a client's web application would scale to meet demand was better than scaling continuously based on predicted traffic. An auto-scaling application would not be able to appropriately scale the websites based on current traffic to meet the needs of future traffic appropriately without considering rush times. Campsite reservations often booked by everyone at a single time, and, without a complex algorithm, it would not be able predict an accelerating arrival of new traffic during peak hours and may under scale often during these rushes. Therefore, I expanded this learning goal to use my newfound knowledge in PowerShell and Azure to tailor solutions to the clients needs based on their own site reservation times. By working on the ticket to move client databases into an elastic pool, I was successful in implementing cost saving measures.</p>
		                		</li>
		                	</ul>
		            </div>
		        </section>
		    </div>
	        <footer>
	        	<p>Jeremiah Wernham 2020</p>
	        </footer>
	    </div>
	</body>

</html>
